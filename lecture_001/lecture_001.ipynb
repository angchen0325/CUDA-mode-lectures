{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjcxLjcgMTc5LjciIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgd2lkdGg9IjI1MDAiIGhlaWdodD0iMTY1MyI+PHBhdGggZD0iTTEwMS4zIDUzLjZWMzcuNGMxLjYtLjEgMy4yLS4yIDQuOC0uMiA0NC40LTEuNCA3My41IDM4LjIgNzMuNSAzOC4yUzE0OC4yIDExOSAxMTQuNSAxMTljLTQuNSAwLTguOS0uNy0xMy4xLTIuMVY2Ny43YzE3LjMgMi4xIDIwLjggOS43IDMxLjEgMjdsMjMuMS0xOS40cy0xNi45LTIyLjEtNDUuMy0yMi4xYy0zLS4xLTYgLjEtOSAuNG0wLTUzLjZ2MjQuMmw0LjgtLjNjNjEuNy0yLjEgMTAyIDUwLjYgMTAyIDUwLjZzLTQ2LjIgNTYuMi05NC4zIDU2LjJjLTQuMiAwLTguMy0uNC0xMi40LTEuMXYxNWMzLjQuNCA2LjkuNyAxMC4zLjcgNDQuOCAwIDc3LjItMjIuOSAxMDguNi00OS45IDUuMiA0LjIgMjYuNSAxNC4zIDMwLjkgMTguNy0yOS44IDI1LTk5LjMgNDUuMS0xMzguNyA0NS4xLTMuOCAwLTcuNC0uMi0xMS0uNnYyMS4xaDE3MC4yVjBIMTAxLjN6bTAgMTE2Ljl2MTIuOGMtNDEuNC03LjQtNTIuOS01MC41LTUyLjktNTAuNXMxOS45LTIyIDUyLjktMjUuNnYxNGgtLjFjLTE3LjMtMi4xLTMwLjkgMTQuMS0zMC45IDE0LjFzNy43IDI3LjMgMzEgMzUuMk0yNy44IDc3LjRzMjQuNS0zNi4yIDczLjYtNDBWMjQuMkM0NyAyOC42IDAgNzQuNiAwIDc0LjZzMjYuNiA3NyAxMDEuMyA4NHYtMTRjLTU0LjgtNi44LTczLjUtNjcuMi03My41LTY3LjJ6IiBmaWxsPSIjNzZiOTAwIi8+PC9zdmc+\" style=\"width:8.5%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=5 face=\"Helvetica\" color=#76B900><b>\n",
    "CUDA MODE: Lecture 1\n",
    "</b></font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font face=\"Helvetica\" size=3><b>Ang Chen</b></font></center>\n",
    "<center><font face=\"Helvetica\" size=3>September, 2024</font></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1., 2., 3.])\n",
    "\n",
    "print(torch.square(a))\n",
    "print(a ** 2)\n",
    "print(a * a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pytorch_function(func, input):\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(input)\n",
    "\n",
    "    start.record()\n",
    "    func(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(10000, 10000).cuda()\n",
    "\n",
    "def square_2(a):\n",
    "    return a * a\n",
    "\n",
    "def square_3(a):\n",
    "    return a ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Profiling torch.square\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "time_pytorch_function(torch.square, b)\n",
    "time_pytorch_function(square_2, b)\n",
    "time_pytorch_function(square_3, b)\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling torch.square\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         aten::square        22.83%     189.000us       100.00%     828.000us     828.000us     128.000us         0.12%     107.373ms     107.373ms             1  \n",
      "            aten::pow        76.93%     637.000us        77.17%     639.000us     639.000us     107.240ms        99.88%     107.245ms     107.245ms             1  \n",
      "    aten::result_type         0.12%       1.000us         0.12%       1.000us       1.000us       3.000us         0.00%       3.000us       3.000us             1  \n",
      "             aten::to         0.12%       1.000us         0.12%       1.000us       1.000us       2.000us         0.00%       2.000us       2.000us             1  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 828.000us\n",
      "Self CUDA time total: 107.373ms\n",
      "\n",
      "=============\n",
      "Profiling a * a\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    torch.square(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a * a\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    aten::mul       100.00%      52.000us       100.00%      52.000us      52.000us     121.031ms       100.00%     121.031ms     121.031ms             1  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 52.000us\n",
      "Self CUDA time total: 121.031ms\n",
      "\n",
      "=============\n",
      "Profiling a ** 2\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_2(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a ** 2\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "            aten::pow        97.01%      65.000us       100.00%      67.000us      67.000us     121.912ms       100.00%     121.915ms     121.915ms             1  \n",
      "             aten::to         1.49%       1.000us         1.49%       1.000us       1.000us       2.000us         0.00%       2.000us       2.000us             1  \n",
      "    aten::result_type         1.49%       1.000us         1.49%       1.000us       1.000us       1.000us         0.00%       1.000us       1.000us             1  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 67.000us\n",
      "Self CUDA time total: 121.915ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_3(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
