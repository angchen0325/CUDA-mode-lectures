{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjcxLjcgMTc5LjciIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgd2lkdGg9IjI1MDAiIGhlaWdodD0iMTY1MyI+PHBhdGggZD0iTTEwMS4zIDUzLjZWMzcuNGMxLjYtLjEgMy4yLS4yIDQuOC0uMiA0NC40LTEuNCA3My41IDM4LjIgNzMuNSAzOC4yUzE0OC4yIDExOSAxMTQuNSAxMTljLTQuNSAwLTguOS0uNy0xMy4xLTIuMVY2Ny43YzE3LjMgMi4xIDIwLjggOS43IDMxLjEgMjdsMjMuMS0xOS40cy0xNi45LTIyLjEtNDUuMy0yMi4xYy0zLS4xLTYgLjEtOSAuNG0wLTUzLjZ2MjQuMmw0LjgtLjNjNjEuNy0yLjEgMTAyIDUwLjYgMTAyIDUwLjZzLTQ2LjIgNTYuMi05NC4zIDU2LjJjLTQuMiAwLTguMy0uNC0xMi40LTEuMXYxNWMzLjQuNCA2LjkuNyAxMC4zLjcgNDQuOCAwIDc3LjItMjIuOSAxMDguNi00OS45IDUuMiA0LjIgMjYuNSAxNC4zIDMwLjkgMTguNy0yOS44IDI1LTk5LjMgNDUuMS0xMzguNyA0NS4xLTMuOCAwLTcuNC0uMi0xMS0uNnYyMS4xaDE3MC4yVjBIMTAxLjN6bTAgMTE2Ljl2MTIuOGMtNDEuNC03LjQtNTIuOS01MC41LTUyLjktNTAuNXMxOS45LTIyIDUyLjktMjUuNnYxNGgtLjFjLTE3LjMtMi4xLTMwLjkgMTQuMS0zMC45IDE0LjFzNy43IDI3LjMgMzEgMzUuMk0yNy44IDc3LjRzMjQuNS0zNi4yIDczLjYtNDBWMjQuMkM0NyAyOC42IDAgNzQuNiAwIDc0LjZzMjYuNiA3NyAxMDEuMyA4NHYtMTRjLTU0LjgtNi44LTczLjUtNjcuMi03My41LTY3LjJ6IiBmaWxsPSIjNzZiOTAwIi8+PC9zdmc+\" style=\"width:8.5%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=5 face=\"Helvetica\" color=#76B900><b>\n",
    "CUDA MODE: Lecture 1\n",
    "</b></font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font face=\"Helvetica\" size=3><b>Ang Chen</b></font></center>\n",
    "<center><font face=\"Helvetica\" size=3>September, 2024</font></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1., 2., 3.])\n",
    "\n",
    "print(torch.square(a))\n",
    "print(a ** 2)\n",
    "print(a * a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pytorch_function(func, input):\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(input)\n",
    "\n",
    "    start.record()\n",
    "    func(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(10000, 10000).cuda()\n",
    "\n",
    "def square_2(a):\n",
    "    return a * a\n",
    "\n",
    "def square_3(a):\n",
    "    return a ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Profiling torch.square\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "time_pytorch_function(torch.square, b)\n",
    "time_pytorch_function(square_2, b)\n",
    "time_pytorch_function(square_3, b)\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling torch.square\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         aten::square        11.85%     156.000us       100.00%       1.316ms       1.316ms     155.000us         5.65%       2.744ms       2.744ms             1  \n",
      "            aten::pow        87.99%       1.158ms        88.15%       1.160ms       1.160ms       2.574ms        93.80%       2.589ms       2.589ms             1  \n",
      "    aten::result_type         0.08%       1.000us         0.08%       1.000us       1.000us       8.000us         0.29%       8.000us       8.000us             1  \n",
      "             aten::to         0.08%       1.000us         0.08%       1.000us       1.000us       7.000us         0.26%       7.000us       7.000us             1  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.316ms\n",
      "Self CUDA time total: 2.744ms\n",
      "\n",
      "=============\n",
      "Profiling a * a\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    torch.square(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a * a\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    aten::mul       100.00%      51.000us       100.00%      51.000us      51.000us       1.316ms       100.00%       1.316ms       1.316ms             1  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 51.000us\n",
      "Self CUDA time total: 1.316ms\n",
      "\n",
      "=============\n",
      "Profiling a ** 2\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_2(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a ** 2\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "            aten::pow        98.15%     106.000us       100.00%     108.000us     108.000us       1.961ms        99.19%       1.977ms       1.977ms             1  \n",
      "    aten::result_type         0.93%       1.000us         0.93%       1.000us       1.000us       8.000us         0.40%       8.000us       8.000us             1  \n",
      "             aten::to         0.93%       1.000us         0.93%       1.000us       1.000us       8.000us         0.40%       8.000us       8.000us             1  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 108.000us\n",
      "Self CUDA time total: 1.977ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_3(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello load inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_source = \"\"\"\n",
    "std::string hello_world() {\n",
    "    return \"Hello World!\";\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emitting ninja build file ./tmp\\build.ninja...\n",
      "Building extension module my_module...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module my_module...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello World!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_module = load_inline(\n",
    "    name='my_module',\n",
    "    cpp_sources=[cpp_source],\n",
    "    functions=['hello_world'],\n",
    "    verbose=True,\n",
    "    build_directory='./tmp'\n",
    ")\n",
    "\n",
    "my_module.hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CUDA kernel and C++ wrapper\n",
    "cuda_source = \"\"\"\n",
    "__global__ void square_matrix_kernel(const float* matrix, float* result, int width, int height) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < height && col < width) {\n",
    "        int idx = row * width + col;\n",
    "        result[idx] = matrix[idx] * matrix[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor square_matrix(torch::Tensor matrix) {\n",
    "    const auto height = matrix.size(0);\n",
    "    const auto width = matrix.size(1);\n",
    "\n",
    "    auto result = torch::empty_like(matrix);\n",
    "\n",
    "    dim3 threads_per_block(16, 16);\n",
    "    dim3 number_of_blocks((width + threads_per_block.x - 1) / threads_per_block.x,\n",
    "                          (height + threads_per_block.y - 1) / threads_per_block.y);\n",
    "\n",
    "    square_matrix_kernel<<<number_of_blocks, threads_per_block>>>(\n",
    "        matrix.data_ptr<float>(), result.data_ptr<float>(), width, height);\n",
    "\n",
    "    return result;\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "cpp_source = \"torch::Tensor square_matrix(torch::Tensor matrix);\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chena\\VenvPy\\cuda\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the CUDA kernel as a PyTorch extension\n",
    "square_matrix_extension = load_inline(\n",
    "    name='square_matrix_extension',\n",
    "    cpp_sources=cpp_source,\n",
    "    cuda_sources=cuda_source,\n",
    "    functions=['square_matrix'],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    build_directory='./load_inline_cuda',\n",
    "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device='cuda')\n",
    "print(square_matrix_extension.square_matrix(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def square_matrix_kernel(matrix, result):\n",
    "    # Calculate the row and column index for each thread\n",
    "    row, col = cuda.grid(2)\n",
    "\n",
    "    # Check if the thread's indices are within the bounds of the matrix\n",
    "    if row < matrix.shape[0] and col < matrix.shape[1]:\n",
    "        # Perform the square operation\n",
    "        result[row, col] = matrix[row, col] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample matrix\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n",
    "\n",
    "# Allocate memory on the device\n",
    "d_matrix = cuda.to_device(matrix)\n",
    "d_result = cuda.device_array(matrix.shape, dtype=np.float32)\n",
    "\n",
    "# Configure the blocks\n",
    "threads_per_block = (16, 16)\n",
    "blocks_per_grid_x = int(np.ceil(matrix.shape[0] / threads_per_block[0]))\n",
    "blocks_per_grid_y = int(np.ceil(matrix.shape[1] / threads_per_block[1]))\n",
    "blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chena\\VenvPy\\cuda\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# Launch the kernel\n",
    "square_matrix_kernel[blocks_per_grid, threads_per_block](d_matrix, d_result)\n",
    "\n",
    "# Copy the result back to the host\n",
    "result = d_result.copy_to_host()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result is now in 'result' array\n",
    "print(matrix)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def square_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    # The rows of the softmax are independent, so we parallelize across those\n",
    "    row_idx = tl.program_id(0)\n",
    "    # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "    # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "    # row in a single block\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    input_ptrs = row_start_ptr + col_offsets\n",
    "    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n",
    "\n",
    "    square_output = row * row\n",
    "    \n",
    "    # Write back output to DRAM\n",
    "    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "    output_ptrs = output_row_start_ptr + col_offsets\n",
    "    tl.store(output_ptrs, square_output, mask=col_offsets < n_cols)\n",
    "\n",
    "\n",
    "def square(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "    # The block size is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "    # Another trick we can use is to ask the compiler to use more threads per row by\n",
    "    # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # You will see in the next tutorial how to auto-tune this value in a more natural\n",
    "    # way so you don't have to come up with manual heuristics yourself.\n",
    "    num_warps = 4\n",
    "    if BLOCK_SIZE >= 2048:\n",
    "        num_warps = 8\n",
    "    if BLOCK_SIZE >= 4096:\n",
    "        num_warps = 16\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n",
    "    # f the input matrix\n",
    "    square_kernel[(n_rows, )](\n",
    "        y,\n",
    "        x,\n",
    "        x.stride(0),\n",
    "        y.stride(0),\n",
    "        n_cols,\n",
    "        num_warps=num_warps,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.41.34120\\\\bin\\\\Hostx64\\\\x64\\\\cl.EXE', 'C:\\\\Users\\\\chena\\\\AppData\\\\Local\\\\Temp\\\\tmpkrlb0yzh\\\\main.c', '/nologo', '/O2', '/LD', '/wd4819', '/IC:\\\\Users\\\\chena\\\\VenvPy\\\\cuda\\\\Lib\\\\site-packages\\\\triton\\\\backends\\\\nvidia\\\\include', '/IC:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\include', '/IC:\\\\Users\\\\chena\\\\AppData\\\\Local\\\\Temp\\\\tmpkrlb0yzh', '/IC:\\\\Python3\\\\Python311\\\\Include', '/IC:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.41.34120\\\\include', '/IC:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Include\\\\10.0.22621.0\\\\shared', '/IC:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Include\\\\10.0.22621.0\\\\ucrt', '/IC:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Include\\\\10.0.22621.0\\\\um', '/link', '/LIBPATH:C:\\\\Users\\\\chena\\\\VenvPy\\\\cuda\\\\Lib\\\\site-packages\\\\triton\\\\backends\\\\nvidia\\\\lib', '/LIBPATH:C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\lib\\\\x64', '/LIBPATH:c:\\\\Users\\\\chena\\\\VenvPy\\\\cuda\\\\libs', '/LIBPATH:c:\\\\Users\\\\chena\\\\VenvPy\\\\cuda\\\\Scripts\\\\libs', '/LIBPATH:C:\\\\Python311\\\\libs', '/LIBPATH:C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.41.34120\\\\lib\\\\x64', '/LIBPATH:C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Lib\\\\10.0.22621.0\\\\ucrt\\\\x64', '/LIBPATH:C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Lib\\\\10.0.22621.0\\\\um\\\\x64', 'cuda.lib', '/OUT:C:\\\\Users\\\\chena\\\\AppData\\\\Local\\\\Temp\\\\tmpkrlb0yzh\\\\cuda_utils.cp311-win_amd64.pyd']' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1823\u001b[39m, \u001b[38;5;241m781\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m y_triton \u001b[38;5;241m=\u001b[39m \u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m y_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msquare(x)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(y_triton, y_torch), (y_triton, y_torch)\n",
      "Cell \u001b[1;32mIn[18], line 39\u001b[0m, in \u001b[0;36msquare\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     36\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty_like(x)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# f the input matrix\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43msquare_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_warps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_warps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\chena\\VenvPy\\cuda\\Lib\\site-packages\\triton\\runtime\\jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chena\\VenvPy\\cuda\\Lib\\site-packages\\triton\\runtime\\jit.py:607\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[1;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, grid, warmup, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;66;03m# parse options\u001b[39;00m\n\u001b[1;32m--> 607\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_device\u001b[49m()\n\u001b[0;32m    608\u001b[0m     stream \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mactive\u001b[38;5;241m.\u001b[39mget_current_stream(device)\n\u001b[0;32m    609\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug\n",
      "File \u001b[1;32mc:\\Users\\chena\\VenvPy\\cuda\\Lib\\site-packages\\triton\\runtime\\driver.py:23\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj, name)\n",
      "File \u001b[1;32mc:\\Users\\chena\\VenvPy\\cuda\\Lib\\site-packages\\triton\\runtime\\driver.py:20\u001b[0m, in \u001b[0;36mLazyProxy._initialize_obj\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chena\\VenvPy\\cuda\\Lib\\site-packages\\triton\\runtime\\driver.py:9\u001b[0m, in \u001b[0;36m_create_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(actives) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(actives)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m active drivers (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactives\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). There should only be one.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactives\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chena\\VenvPy\\cuda\\Lib\\site-packages\\triton\\backends\\nvidia\\driver.py:412\u001b[0m, in \u001b[0;36mCudaDriver.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutils \u001b[38;5;241m=\u001b[39m \u001b[43mCudaUtils\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TODO: make static\u001b[39;00m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlauncher_cls \u001b[38;5;241m=\u001b[39m CudaLauncher\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\chena\\VenvPy\\cuda\\Lib\\site-packages\\triton\\backends\\nvidia\\driver.py:90\u001b[0m, in \u001b[0;36mCudaUtils.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 90\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_module_from_src\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver.c\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda_utils\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_binary \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mload_binary\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_device_properties \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mget_device_properties\n",
      "File \u001b[1;32mc:\\Users\\chena\\VenvPy\\cuda\\Lib\\site-packages\\triton\\backends\\nvidia\\driver.py:67\u001b[0m, in \u001b[0;36mcompile_module_from_src\u001b[1;34m(src, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     66\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(src)\n\u001b[1;32m---> 67\u001b[0m so \u001b[38;5;241m=\u001b[39m \u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibrary_dirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibraries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(so, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     69\u001b[0m     cache_path \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mput(f\u001b[38;5;241m.\u001b[39mread(), so_name, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\chena\\VenvPy\\cuda\\Lib\\site-packages\\triton\\runtime\\build.py:79\u001b[0m, in \u001b[0;36m_build\u001b[1;34m(name, src, srcdir, library_dirs, include_dirs, libraries)\u001b[0m\n\u001b[0;32m     77\u001b[0m     library_dirs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m msvc_winsdk_lib_dirs\n\u001b[0;32m     78\u001b[0m cc_cmd \u001b[38;5;241m=\u001b[39m _cc_cmd(cc, src, so, include_dirs, library_dirs, libraries)\n\u001b[1;32m---> 79\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m so\n",
      "File \u001b[1;32mC:\\Python3\\Python311\\Lib\\subprocess.py:413\u001b[0m, in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cmd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    412\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m popenargs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.41.34120\\\\bin\\\\Hostx64\\\\x64\\\\cl.EXE', 'C:\\\\Users\\\\chena\\\\AppData\\\\Local\\\\Temp\\\\tmpkrlb0yzh\\\\main.c', '/nologo', '/O2', '/LD', '/wd4819', '/IC:\\\\Users\\\\chena\\\\VenvPy\\\\cuda\\\\Lib\\\\site-packages\\\\triton\\\\backends\\\\nvidia\\\\include', '/IC:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\include', '/IC:\\\\Users\\\\chena\\\\AppData\\\\Local\\\\Temp\\\\tmpkrlb0yzh', '/IC:\\\\Python3\\\\Python311\\\\Include', '/IC:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.41.34120\\\\include', '/IC:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Include\\\\10.0.22621.0\\\\shared', '/IC:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Include\\\\10.0.22621.0\\\\ucrt', '/IC:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Include\\\\10.0.22621.0\\\\um', '/link', '/LIBPATH:C:\\\\Users\\\\chena\\\\VenvPy\\\\cuda\\\\Lib\\\\site-packages\\\\triton\\\\backends\\\\nvidia\\\\lib', '/LIBPATH:C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5\\\\lib\\\\x64', '/LIBPATH:c:\\\\Users\\\\chena\\\\VenvPy\\\\cuda\\\\libs', '/LIBPATH:c:\\\\Users\\\\chena\\\\VenvPy\\\\cuda\\\\Scripts\\\\libs', '/LIBPATH:C:\\\\Python311\\\\libs', '/LIBPATH:C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.41.34120\\\\lib\\\\x64', '/LIBPATH:C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Lib\\\\10.0.22621.0\\\\ucrt\\\\x64', '/LIBPATH:C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Lib\\\\10.0.22621.0\\\\um\\\\x64', 'cuda.lib', '/OUT:C:\\\\Users\\\\chena\\\\AppData\\\\Local\\\\Temp\\\\tmpkrlb0yzh\\\\cuda_utils.cp311-win_amd64.pyd']' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(1823, 781, device='cuda')\n",
    "y_triton = square(x)\n",
    "y_torch = torch.square(x)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know much about $\\texttt{triton}$.\n",
    "Continue to learn other lectures of the course, and come back to $\\texttt{triton}$ in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
