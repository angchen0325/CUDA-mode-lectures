{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjcxLjcgMTc5LjciIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgd2lkdGg9IjI1MDAiIGhlaWdodD0iMTY1MyI+PHBhdGggZD0iTTEwMS4zIDUzLjZWMzcuNGMxLjYtLjEgMy4yLS4yIDQuOC0uMiA0NC40LTEuNCA3My41IDM4LjIgNzMuNSAzOC4yUzE0OC4yIDExOSAxMTQuNSAxMTljLTQuNSAwLTguOS0uNy0xMy4xLTIuMVY2Ny43YzE3LjMgMi4xIDIwLjggOS43IDMxLjEgMjdsMjMuMS0xOS40cy0xNi45LTIyLjEtNDUuMy0yMi4xYy0zLS4xLTYgLjEtOSAuNG0wLTUzLjZ2MjQuMmw0LjgtLjNjNjEuNy0yLjEgMTAyIDUwLjYgMTAyIDUwLjZzLTQ2LjIgNTYuMi05NC4zIDU2LjJjLTQuMiAwLTguMy0uNC0xMi40LTEuMXYxNWMzLjQuNCA2LjkuNyAxMC4zLjcgNDQuOCAwIDc3LjItMjIuOSAxMDguNi00OS45IDUuMiA0LjIgMjYuNSAxNC4zIDMwLjkgMTguNy0yOS44IDI1LTk5LjMgNDUuMS0xMzguNyA0NS4xLTMuOCAwLTcuNC0uMi0xMS0uNnYyMS4xaDE3MC4yVjBIMTAxLjN6bTAgMTE2Ljl2MTIuOGMtNDEuNC03LjQtNTIuOS01MC41LTUyLjktNTAuNXMxOS45LTIyIDUyLjktMjUuNnYxNGgtLjFjLTE3LjMtMi4xLTMwLjkgMTQuMS0zMC45IDE0LjFzNy43IDI3LjMgMzEgMzUuMk0yNy44IDc3LjRzMjQuNS0zNi4yIDczLjYtNDBWMjQuMkM0NyAyOC42IDAgNzQuNiAwIDc0LjZzMjYuNiA3NyAxMDEuMyA4NHYtMTRjLTU0LjgtNi44LTczLjUtNjcuMi03My41LTY3LjJ6IiBmaWxsPSIjNzZiOTAwIi8+PC9zdmc+\" style=\"width:8.5%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=5 face=\"Helvetica\" color=#76B900><b>\n",
    "CUDA MODE: Lecture 1\n",
    "</b></font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font face=\"Helvetica\" size=3><b>Ang Chen</b></font></center>\n",
    "<center><font face=\"Helvetica\" size=3>September, 2024</font></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1., 2., 3.])\n",
    "\n",
    "print(torch.square(a))\n",
    "print(a ** 2)\n",
    "print(a * a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pytorch_function(func, input):\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(input)\n",
    "\n",
    "    start.record()\n",
    "    func(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(10000, 10000).cuda()\n",
    "\n",
    "def square_2(a):\n",
    "    return a * a\n",
    "\n",
    "def square_3(a):\n",
    "    return a ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Profiling torch.square\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "time_pytorch_function(torch.square, b)\n",
    "time_pytorch_function(square_2, b)\n",
    "time_pytorch_function(square_3, b)\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling torch.square\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         aten::square        26.42%     214.000us       100.00%     810.000us     810.000us     587.000us        13.80%       4.254ms       4.254ms             1  \n",
      "            aten::pow        73.33%     594.000us        73.58%     596.000us     596.000us       3.665ms        86.15%       3.667ms       3.667ms             1  \n",
      "    aten::result_type         0.12%       1.000us         0.12%       1.000us       1.000us       1.000us         0.02%       1.000us       1.000us             1  \n",
      "             aten::to         0.12%       1.000us         0.12%       1.000us       1.000us       1.000us         0.02%       1.000us       1.000us             1  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 810.000us\n",
      "Self CUDA time total: 4.254ms\n",
      "\n",
      "=============\n",
      "Profiling a * a\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    torch.square(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a * a\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    aten::mul       100.00%      49.000us       100.00%      49.000us      49.000us       3.528ms       100.00%       3.528ms       3.528ms             1  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 49.000us\n",
      "Self CUDA time total: 3.528ms\n",
      "\n",
      "=============\n",
      "Profiling a ** 2\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_2(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a ** 2\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "            aten::pow        97.33%      73.000us       100.00%      75.000us      75.000us       3.118ms        99.94%       3.120ms       3.120ms             1  \n",
      "    aten::result_type         1.33%       1.000us         1.33%       1.000us       1.000us       1.000us         0.03%       1.000us       1.000us             1  \n",
      "             aten::to         1.33%       1.000us         1.33%       1.000us       1.000us       1.000us         0.03%       1.000us       1.000us             1  \n",
      "---------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 75.000us\n",
      "Self CUDA time total: 3.120ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    square_3(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello load inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_source = \"\"\"\n",
    "std::string hello_world() {\n",
    "    return \"Hello World!\";\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emitting ninja build file ./tmp\\build.ninja...\n",
      "Building extension module my_module...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module my_module...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello World!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_module = load_inline(\n",
    "    name='my_module',\n",
    "    cpp_sources=[cpp_source],\n",
    "    functions=['hello_world'],\n",
    "    verbose=True,\n",
    "    build_directory='./tmp'\n",
    ")\n",
    "\n",
    "my_module.hello_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
